<<<<<<< HEAD
import os
import torch
import torchaudio
import numpy as np
from transformers import Wav2Vec2Processor, Wav2Vec2ForSequenceClassification

# ðŸ§  ë°±ì—”ë“œ ì„¤ì •
torchaudio.set_audio_backend("soundfile")

# ðŸ“‚ ëª¨ë¸ ë””ë ‰í† ë¦¬ ê²½ë¡œ
MODEL_DIR = os.path.dirname(os.path.abspath(__file__))

# â¬‡ï¸ ëª¨ë¸ ë° ì „ì²˜ë¦¬ê¸° ë¡œë”©
processor = Wav2Vec2Processor.from_pretrained(MODEL_DIR)
model = Wav2Vec2ForSequenceClassification.from_pretrained(MODEL_DIR)
model.eval()

NUM_CLASSES = model.config.num_labels  # ë³´í†µ 2ê°œ (ì •ìƒ, ë¹„ì •ìƒ)

def predict_audio(waveform: torch.Tensor, sample_rate: int) -> dict:
    print(f"[ðŸ§ ] ì˜ˆì¸¡ ì‹œìž‘ - waveform.shape={waveform.shape}, sample_rate={sample_rate}")

    # ë¦¬ìƒ˜í”Œë§ (16kHz ë§žì¶¤)
    if sample_rate != 16000:
        resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)
        waveform = resampler(waveform)

    # numpyë¡œ ë³€í™˜
    if isinstance(waveform, torch.Tensor):
        waveform = waveform.squeeze().numpy()

    # ì „ì²˜ë¦¬
    inputs = processor(waveform, sampling_rate=16000, return_tensors="pt", padding=True)

    # ì˜ˆì¸¡
    with torch.no_grad():
        logits = model(**inputs).logits
        probs = torch.softmax(logits, dim=-1).squeeze()

    predicted_class = int(torch.argmax(probs))
    return {
        "class": predicted_class,
        "probabilities": probs.tolist()
    }

def predict_average(file_paths: list[str]) -> dict:
    print(f"[ðŸ“Š] ì´ {len(file_paths)}ê°œ íŒŒì¼ í‰ê·  ì˜ˆì¸¡ ì‹œìž‘")
    all_probs = []

    for path in file_paths:
        try:
            waveform, sample_rate = torchaudio.load(path)
            result = predict_audio(waveform, sample_rate)
            all_probs.append(result["probabilities"])
            print(f"[âœ…] {os.path.basename(path)} â†’ {result}")
        except Exception as e:
            print(f"[âŒ] ì˜ˆì¸¡ ì‹¤íŒ¨ - {path}: {e}")

    if not all_probs:
        return {"error": "ì˜ˆì¸¡ ê°€ëŠ¥í•œ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤."}

    avg_probs = torch.tensor(all_probs).mean(dim=0)
    final_class = int(torch.argmax(avg_probs))

    return {
        "average_probabilities": avg_probs.tolist(),
        "predicted_class": final_class
    }
=======
from transformers import AutoProcessor, AutoModelForSequenceClassification
import torchaudio
import torch

MODEL_DIR = "backend/model"  # ëª¨ë¸ íŒŒì¼ë“¤ì´ ì €ìž¥ëœ ê²½ë¡œ

# ëª¨ë¸ ë° ì „ì²˜ë¦¬ê¸° ë¶ˆëŸ¬ì˜¤ê¸°
processor = AutoProcessor.from_pretrained(MODEL_DIR)
model = AutoModelForSequenceClassification.from_pretrained(MODEL_DIR)
model.eval()

def predict_audio(wav_path: str) -> int:
    waveform, sample_rate = torchaudio.load(wav_path)
    inputs = processor(
        waveform.squeeze().numpy(),
        sampling_rate=sample_rate,
        return_tensors="pt",
        padding=True
    )
    with torch.no_grad():
        logits = model(**inputs).logits
        prediction = torch.argmax(logits, dim=-1).item()
    return prediction
>>>>>>> 57a5bf9f8c14016fc41945ad1ba65cfb2e7542c3
